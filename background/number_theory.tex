%BEGIN LICS
\section{Mathematical Background}
\label{background}

\subsection{Number Theory and Diophantine Approximation}

A number $\alpha \in \mathbb{C}$ is said to be \emph{algebraic} if
there exists a non-zero polynomial $p \in \mathbb{Q}[x]$ for which
$p(\alpha) = 0$. A complex number that is not algebraic is said to be
\emph{transcendental}. The monic polynomial $p \in \mathbb{Q}[x]$ of
smallest degree for which $p(\alpha) = 0$ is said to be the minimal
polynomial of $\alpha$. The set of algebraic numbers, denoted by
$\overline{\mathbb{Q}}$, forms a field. Note that the complex
conjugate of an algebraic number is also algebraic, with the same
minimal polynomial. It is possible to represent and manipulate
algebraic numbers effectively, by storing their minimal polynomial and
a sufficiently precise numerical approximation. An excellent course
(and reference) in computational algebraic number theory can be found
in \cite{Cohen}. Efficient algorithms for approximating algebraic
numbers were presented in \cite{Pan}.

Given a vector $\boldsymbol{\lambda} \in \overline{\mathbb{Q}}^{m}$, its \emph{group of multiplicative relations} is defined as
\begin{align*}
L(\boldsymbol{\lambda}) = \lbrace \boldsymbol{v} \in \mathbb{Z}^{m} : \boldsymbol{\lambda}^{\boldsymbol{v}} = 1 \rbrace .
\end{align*}

Moreover, letting $\log$ represent a fixed branch of the complex logarithm function, note that $\log(\alpha_{1}), \ldots, \log(\alpha_{m})$ are linearly independent over $\mathbb{Q}$ if and only if
\begin{align*}
L(\alpha_{1}, \ldots, \alpha_{m}) = \lbrace \boldsymbol{0} \rbrace .
\end{align*}

Being a subgroup of the free finitely generated abelian group $\mathbb{Z}^{m}$, the group $L(\boldsymbol{\lambda})$ is also free and admits a finite basis.

The following theorem, due to David Masser, allows us to effectively determine $L(\boldsymbol{\lambda})$, and in particular decide whether it is equal to $\lbrace \boldsymbol{0} \rbrace$. This result can be found in \cite{Masser}.

\begin{theorem}[Masser]
The free abelian group $L(\boldsymbol{\lambda})$ has a basis $\boldsymbol{v}_{1}, \ldots, \boldsymbol{v}_{l} \in \mathbb{Z}^{m}$ for which
\begin{align*}
\max\limits_{1 \leq i \leq l, 1 \leq j \leq m} \lvert v_{i,j} \rvert \leq (D \log H)^{O(m^{2})}
\end{align*}
where $H$ and $D$ bound respectively the heights and degrees of all the $\lambda_{i}$.
\end{theorem}

%We will need the following results of Baker~\cite{Baker75}.
%The first one, together with Masser's theorem, allows us
%to eliminate all algebraic relations in the description of linear
%forms in logarithms of algebraic numbers.

Together with the following result, due to Alan Baker, Masser's theorem allows us to eliminate all algebraic relations in the description of linear forms in logarithms of algebraic numbers. In particular, it also yields a method for comparing linear forms in logarithms of algebraic numbers: test whether their difference is zero and, if not, approximate it numerically to sufficient precision, so as to infer its sign. Note that the set of linear forms in logarithms of algebraic numbers is closed under addition and under multiplication by algebraic numbers, as well as under complex conjugation. See \cite{Baker75} and \cite{BakerPaper}.

\begin{theorem}[Baker]
Let $\alpha_{1}, \ldots, \alpha_{m} \in \overline{\mathbb{Q}} \setminus \lbrace 0 \rbrace$. If
\begin{align*}
\log(\alpha_{1}), \ldots, \log(\alpha_{m})
\end{align*}
are linearly independent over $\mathbb{Q}$, then
\begin{align*}
1, \log(\alpha_{1}), \ldots, \log(\alpha_{m})
\end{align*}
are linearly independent over $\overline{\mathbb{Q}}$.
\end{theorem}

%The next result essentially implies that one can effectively check
%whether a linear form in logarithms of algebraic numbers equals
%zero. Noting that the set of linear forms in logarithms of algebraic
%numbers is closed under addition and multiplication by algebraic
%numbers, it easily follows that one can effectively compare two linear
%forms in logarithms of algebraic numbers. It is also closed under
%complex conjugation. See \cite{Baker75} and \cite{BakerPaper}.

%\begin{theorem}[Baker]
%Let $\alpha_{1}, \ldots, \alpha_{m}$ be non-zero algebraic numbers with degrees at most $d$ and heights at most $A$. Further, let $\beta_{0}, \ldots, \beta_{m}$ be algebraic numbers with degrees at most $d$ and heights at most $B$, where $B \geq 2$. Write
%\begin{align*}
%\Lambda = \beta_{0} + \beta_{1} \log(\alpha_{1}) + \cdots + \beta_{m} \log(\alpha_{m}) .
%\end{align*}
%Then either $\Lambda = 0$ or $\lvert \Lambda \rvert > B^{-C}$, where $C$ is an effectively computable number depending only on $m$, $d$, $A$, and the chosen branch of the complex logarithm.
%\end{theorem}

The theorem below was proved by Ferdinand von Lindemann in 1882, and later generalised by Karl Weierstrass in what is now known as the Lindemann-Weierstrass theorem. As a historical note, this result was behind the first proof of transcendence of $\pi$, which immediately follows from it.

\begin{theorem}[Lindemann]
If $\alpha \in \overline{\mathbb{Q}} \setminus \lbrace 0 \rbrace$, then $e^{\alpha}$ is transcendental.
\end{theorem}

We will also need the following result, due to Leopold Kronecker, on simultaneous Diophantine approximation, which generalises Dirichlet's Approximation Theorem. We denote the \emph{group of additive relations} of $\boldsymbol{v}$ by
\begin{align*}
A(\boldsymbol{v}) = \lbrace \boldsymbol{z} \in \mathbb{Z}^{d} : \boldsymbol{z} \cdot \boldsymbol{v} \in \mathbb{Z} \rbrace .
\end{align*}

Throughout this paper, $\operatorname{dist}$ refers to the $l_{1}$ distance.

\begin{theorem}[Kronecker]
\label{Kronecker}
Let $\boldsymbol{\alpha}_{1}, \ldots, \boldsymbol{\alpha_{k}} \in \mathbb{R}^{d}$ and $\boldsymbol{\beta} \in \mathbb{R}^{d}$. The following are equivalent:
\begin{enumerate}
\item For any $\varepsilon > 0$, there exists $\boldsymbol{n} \in \mathbb{N}^{k}$ such that
\begin{align*}
\operatorname{dist}(\boldsymbol{\beta} + \sum\limits_{i=1}^{k} n_{i} \boldsymbol{\alpha}_{i}, \mathbb{Z}^{d}) \leq \varepsilon .
\end{align*}
\item It holds that
\begin{align*}
\bigcap\limits_{i=1}^{k} A(\boldsymbol{\alpha}_{i}) \subseteq A(\boldsymbol{\beta}) .
\end{align*}
\end{enumerate}
\end{theorem}

Many of these results, or slight variations thereof, can be found in \cite{HardyAndWright} and \cite{Cassels}.

\subsection{Lattices}

Consider a non-zero matrix $K\in\overline{\mathbb{Q}}^{r\times d}$ and vector
$\boldsymbol{k} \in \overline{\mathbb{Q}}^r$.  The following proposition shows
how to compute a representation of the affine lattice
$\{ \boldsymbol{x}\in\mathbb{Z}^d : K\boldsymbol{x} = \boldsymbol{k}
\}$.
Further information about lattices can be found in \cite{LatticeBook}
and \cite{Cohen}.

\begin{proposition}
There exist $\boldsymbol{x}_{0} \in \mathbb{Z}^{d}$ and
$M \in \mathbb{Z}^{d \times s}$, where $s < r$, such that
\begin{align*}
  \{ \boldsymbol{x}\in\mathbb{Z}^d : K\boldsymbol{x} =
  \boldsymbol{k} \} =
  \boldsymbol{x}_{0} + \{ M \boldsymbol{y} : \boldsymbol{y} \in \mathbb{Z}^s \} \, .
\end{align*}
\end{proposition}

\begin{proof}
  Let $\theta$ denote a primitive element of the number field
  generated by the entries of $K$ and $\boldsymbol{k}$. Let the degree
  of this extension, which equals the degree of $\theta$, be
  $D$. Then for $\boldsymbol{x} \in \mathbb{Z}^d$ one can write
\begin{align*}
K \boldsymbol{x} = \boldsymbol{k} &\Leftrightarrow \left( \sum \limits_{i=0}^{D-1} N_{i} \theta^{i} \right) \boldsymbol{x} = \sum \limits_{i=0}^{D-1} \boldsymbol{k}_{i} \theta^{i} \\
&\Leftrightarrow N_{i} \boldsymbol{x} = \boldsymbol{k}_{i}, \forall i \in \lbrace 0, \ldots, D-1 \rbrace ,
\end{align*}
for some integer matrices
$N_{0}, \ldots, N_{D-1} \in \mathbb{Z}^{r \times d}$ and integer
vectors
$\boldsymbol{k}_{0}, \ldots, \boldsymbol{k}_{D-1} \in \mathbb{Z}^{r}$.
The solution of each of these equations is clearly an affine lattice, and
therefore so is their intersection.
\end{proof}

\subsection{Matrix exponentials}

Given a matrix $A \in \mathbb{C}^{n \times n}$, its exponential is defined as
\begin{align*}
\exp(A) = \sum \limits_{i=0}^{\infty} \frac{A^{i}}{i!} .
\end{align*}
The series above always converges, and so the exponential of a matrix is always well defined. The standard way of computing $\exp(A)$ is by finding $P \in \mathit{GL}_{n}(\mathbb{C})$ such that $J=P^{-1}AP$ is in Jordan Canonical Form, and by using the fact that $\exp(A) = P \exp(J) P^{-1}$, where $\exp(J)$ is easy to compute. When $A \in \overline{\mathbb{Q}}^{n \times n}$, $P$ can be taken to be in $GL_{n}(\overline{\mathbb{Q}})$; note that

\begin{align*}
\mbox{if } J &= \begin{pmatrix}
\lambda && 1 && 0 && \cdots && 0 \\
0 && \lambda && 1 &&\cdots && 0 \\
\vdots && \vdots && \ddots && \ddots && \vdots \\
0 && 0 && \cdots && \lambda && 1 \\
0 && 0 && \cdots && 0 && \lambda
\end{pmatrix} \mbox{ then } \\
\exp(Jt) &= \exp(\lambda t) \begin{pmatrix}
1 && t && \frac{t^{2}}{2} && \cdots && \frac{t^{k-1}}{(k-1)!} \\
0 && 1 && t && \cdots && \frac{t^{k-2}}{(k-2)!} \\
\vdots && \vdots &&\ddots && \ddots && \vdots \\
0 && 0 && \cdots && 1 && t \\
0 && 0 && \cdots && 0 && 1
\end{pmatrix} .
\end{align*}

Then $\exp(J)$ can be obtained by setting $t=1$, in particular $\exp(J)_{ij} = \frac{\exp(\lambda)}{(j-i)!}$ if $j \geq i$ and $0$ otherwise.

When $A$ and $B$ commute, so must $\exp(A)$ and $\exp(B)$. Moreover, when $A$ and $B$ have algebraic entries, the converse also holds, as shown in \cite{MatrixExps}. Also, when $A$ and $B$ commute, it holds that $\exp(A)\exp(B) = \exp(A+B)$.

\subsection{Matrix logarithms}

The matrix $B$ is said to be a logarithm of the matrix $A$ if $\exp(B) = A$. It is well known that a logarithm of a matrix $A$ exists if and only if $A$ is invertible. However, matrix logarithms need not be unique. In fact, there exist matrices admitting uncountably many logarithms. See, for example, \cite{MatrixLogs1} and \cite{MatrixLogs2}.

A matrix is said to be unitriangular if it is triangular and all its diagonal entries equal $1$. Crucially, the following uniqueness result holds:

\begin{theorem}
\label{logarithm_uniqueness}
Given an upper unitriangular matrix $M \in \mathbb{C}^{n \times n}$, there exists a unique strictly upper triangular matrix $L$ such that $\exp(L)=M$. Moreover, the entries of $L$ lie in the number field $\mathbb{Q}(M_{i,j}: 1 \leq i,j \leq n)$.
\end{theorem}

\begin{proof}
Firstly, we show that, for any strictly upper triangular matrix $T$ and for any $1<m<n$ and $i<j$, the term $(T^{m})_{i,j}$ is polynomial on the elements of the set $\lbrace T_{r,s} : s-r<j-i \rbrace$. This can be seen by induction on $m$, as each $T^{m}$ is strictly upper triangular, and so
\begin{align*}
(T^{m})_{i,j} = \sum\limits_{l=1}^{n} (T^{m-1})_{i,l} T_{l,j} = \sum\limits_{l=i+1}^{j-1} (T^{m-1})_{i,l} T_{l,j} .
\end{align*}

Finally, we show, by induction on $j-i$, that each $L_{i,j}$ is polynomial on the elements of the set
\begin{align*}
\lbrace M_{i,j} \rbrace \cup \lbrace M_{r,s} : s-r < j-i \rbrace .
\end{align*}
If $j-i \leq 0$, then $L_{i,j}=0$, so the claim holds. When $j-i>0$, as $L$ is nilpotent,
\begin{align*}
M_{i,j} &= \exp(L)_{i,j} = L_{i,j} + \sum\limits_{m=2}^{n-1} \frac{1}{m!} (L^{m})_{i,j} \\ \Rightarrow L_{i,j} &= M_{i,j} - \sum\limits_{m=2}^{n-1} \frac{1}{m!} (L^{m})_{i,j} .
\end{align*}
The result now follows from the induction hypothesis and from our previous claim, as this argument can be used to both construct such a matrix $L$ and to prove that it is uniquely determined.
\end{proof}

\subsection{Properties of commuting matrices}

We will now present a useful decomposition of $\mathbb{C}^{n}$ induced by the commuting matrices $A_{1}, \ldots, A_{k} \in \mathbb{C}^{n \times n}$. Let $\sigma(A_{i})$ denote the spectrum of the matrix $A_{i}$. In what follows, let
\begin{align*}
\boldsymbol{\lambda} = (\lambda_{1}, \ldots, \lambda_{k}) \in \sigma(A_{1}) \times \cdots \times \sigma(A_{k}) .
\end{align*}
We remind the reader that $\ker(A_{i} - \lambda_{i})^{n}$ corresponds to the generalised eigenspace of $\lambda_{i}$ of $A_{i}$. Moreover, we define the following subspaces:
\begin{align*}
\mathcal{V}_{\boldsymbol{\lambda}} = \bigcap \limits_{i=1}^{k} \ker(A_{i} - \lambda_{i} I)^{n}.
\end{align*}
Also, let $\Sigma = \lbrace \boldsymbol{\lambda} \in \sigma(A_{1}) \times \cdots \times \sigma(A_{k}) : \mathcal{V}_{\boldsymbol{\lambda}} \neq \lbrace \boldsymbol{0} \rbrace \rbrace$.

\begin{theorem}
\label{subspace_decomposition}
For all $\boldsymbol{\lambda} = (\lambda_{1}, \ldots, \lambda_{k}) \in \Sigma$ and for all $i \in \lbrace 1, \ldots, k \rbrace$, the following properties hold:

\begin{enumerate}

\item $\mathcal{V}_{\boldsymbol{\lambda}}$ is invariant under $A_{i}$.

\item $\sigma(A_{i} \restriction_{\mathcal{V}_{\boldsymbol{\lambda}}}) = \lbrace \lambda_{i} \rbrace$.

\item $\mathbb{C}^{n} = \bigoplus \limits_{\boldsymbol{\lambda} \in \Sigma} \mathcal{V}_{\boldsymbol{\lambda}} .$

\end{enumerate}
\end{theorem}

\begin{proof}
We show, by induction on $k$, that the subspaces $\mathcal{V}_{\boldsymbol{\lambda}}$ satisfy the properties above.

When $k = 1$, the result follows from the existence of Jordan Canonical Forms. When $k > 1$, suppose that $\sigma(A_{k}) = \lbrace \mu_{1}, \ldots, \mu_{m} \rbrace$, and let $\mathcal{U}_{j} = \ker(A_{k} - \mu_{j} I)^{n}$, for $j \in \lbrace 1, \ldots, m \rbrace$. Again, it follows from the existence of Jordan Canonical Forms that
\begin{align*}
\mathbb{C}^{n} = \bigoplus \limits_{j = 1}^{m} \mathcal{U}_{m} .
\end{align*}
In what follows, $i \in \lbrace 1, \ldots, k-1 \rbrace$ and $j \in \lbrace 1, \ldots, m \rbrace$. Now, as $A_{k}$ and $A_{i}$ commute, so do $(A_{k}-\mu_{j} I)$ and $A_{i}$. Therefore, for all $\boldsymbol{v} \in \mathcal{U}_{j}$, $(A_{k} - \mu_{j} I)^{n} A_{i} \boldsymbol{v} = A_{i} (A-\mu_{j} I)^{n} \boldsymbol{v} = \boldsymbol{0}$, so $A_{i} \boldsymbol{v} \in \mathcal{U}_{j}$, that is, $\mathcal{U}_{j}$ is invariant under $A_{i}$. The result follows from applying the induction hypothesis to the commuting operators $A_{i} \restriction_{\mathcal{U}_{j}}$.
\end{proof}

We will also make use of the following well-known result on simultaneous triangularisation of commuting matrices. See, for example, \cite{CommutingMatrices}.

\begin{theorem}
\label{simultaneous-triangularisation}
Given $k$ commuting matrices $A_{1}, \ldots, A_{k} \in \overline{\mathbb{Q}}^{n \times n}$, there exists a matrix $P \in \mathit{GL}_{n}(\overline{\mathbb{Q}})$ such that $P^{-1}A_{i}P$ is upper triangular for all $i \in \lbrace 1, \ldots, k \rbrace$.
\end{theorem}

\subsection{Convex Polyhedra and Semi-Algebraic Sets}

A convex polyhedron is a subset of $\mathbb{R}^{n}$ of the form $\mathcal{P} = \lbrace \boldsymbol{x} \in \mathbb{R}^{n} : A \boldsymbol{x} \leq \boldsymbol{b} \rbrace$, where $A$ is a $d \times n$ matrix and $\boldsymbol{b} \in \mathbb{R}^{d}$. When all the entries of $A$ and coordinates of $\boldsymbol{b}$ are algebraic numbers, the convex polyhedron $\mathcal{P}$ is said to have an algebraic description.

A set $S \subseteq \mathbb{R}^{n}$ is said to be semi-algebraic if it is a Boolean combination of sets of the form $\lbrace \boldsymbol{x} \in \mathbb{R}^{n}: p(\boldsymbol{x}) \geq 0\rbrace$, where $p$ is a polynomial with integer coefficients. Equivalently, the semi-algebraic sets are those definable by the quantifier-free first-order formulas over the structure $(\mathbb{R}, <, +, \cdot, 0, 1)$.

It was shown by Alfred Tarski in \cite{Tarski} that the first-order theory of reals admits quantifier elimination. Therefore, the semi-algebraic sets are precisely the first-order definable sets.

\begin{theorem}[Tarski]
The first-order theory of reals is decidable.
\end{theorem}

See \cite{Renegar} and \cite{BPR06} for more efficient decision procedures for the first-order theory of reals.

\begin{definition}[Hilbert's Tenth Problem]
Given a polynomial $p \in \mathbb{Z}[x_{1}, \ldots, x_{k}]$, decide whether $p(\boldsymbol{x}) = 0$ admits a solution $\boldsymbol{x} \in \mathbb{N}^{k}$. Equivalently, given a semi-algebraic set $S \subseteq \mathbb{R}^{k}$, decide whether it intersects $\mathbb{Z}^{k}$.
\end{definition}

The following celebrated theorem, due to Yuri Matiyasevich, will be used in
our undecidability proof; see \cite{HTP} for a self-contained proof.

\begin{theorem}[Matiyasevich]
Hilbert's Tenth Problem is undecidable.
\end{theorem}

On the other hand, our proof of decidability of ALIP makes use of some techniques present in the proof of the following result, shown in \cite{KP}:

\begin{theorem}[Khachiyan and Porkolab]
It is decidable whether a given \emph{convex} semi-algebraic set $S \subseteq \mathbb{R}^{k}$ intersects $\mathbb{Z}^{k}$.
\end{theorem}

\subsection{Fourier-Motzkin Elimination}

Fourier-Motzkin elimination is a simple method for solving systems of
inequalities. Historically, it was the first algorithm used in solving
linear programming, before more efficient procedures such as the
simplex algorithm were discovered. The procedure consists in isolating one
variable at a time and matching all its lower and upper bounds. Note
that this method preserves the set of solutions on the remaining
variables, so a solution of the reduced system can always be extended
to a solution of the original one.

\begin{theorem}
\label{thm:fme}
  By using Fourier-Motzkin elimination, it is decidable whether a
  given convex polyhedron
  $\mathcal{P} = \lbrace \boldsymbol{x} \in \mathbb{R}^{n} : \pi
  A\boldsymbol{x} < \boldsymbol{b} \rbrace$,
  where the entries of $A$ are all real algebraic numbers and
  those of $\boldsymbol{b}$ are real linear forms in logarithms of
  algebraic numbers, is empty.  Moreover, if $\mathcal{P}$ is
  non-empty one can effectively find a rational vector
  $\boldsymbol{q} \in \mathcal{P}$.
\end{theorem}

\begin{proof}
When using Fourier-Motzkin elimination, isolate each term $\pi x_{i}$, instead of just isolating the variable $x_{i}$. Note that the coefficients of the terms $\pi x_{i}$ will always be algebraic, and the loose constants will always be linear forms in logarithms of algebraic numbers, which are closed under multiplication by algebraic numbers, and which can be effectively compared by using Baker's Theorem.
\end{proof}
%END LICS
